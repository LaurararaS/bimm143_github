---
title: "class08: Mini Project"
author: "Laura Sun (PID: A17923552)"
format: pdf
tov: TRUE
---

## Background

The goal of this mini-project is for you to explore a complete analysis using the unsupervised learning techniques covered in class.

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set first reported by K. P. Benne and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”.

Values in this data set describe characteristics of the cell nuclei present in digitized images of a fine needle aspiration (FNA) of a breast mass.

## Data Import

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
head(wisc.df)
```

The first column `diagnosis` is the expert opinion on the sample (i.e. patient FNA)

```{r}
wisc.data <- wisc.df[,-1]
dim(wisc.data)
```

Store the diagnosis as a vector for use later when we compare our results to those from experts in the field.

```{r}
diagnosis <- factor(wisc.df$diagnosis)
```

> Q1: How many observations are in this dataset?

There are `r nrow(wisc.data)` observations/patients in the dataset.

> Q2: How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```

> Q3: How many variables/features in the data are suffixed with _mean?

```{r}
#colnames(wisc.data), if you want to check the names by eye.
length( grep("_mean", colnames(wisc.data)) )
```

## Principal Component Analysis (PCA)

The `prcomp()` function to PCA. prcomp(x, scale=F, center=F), scale and center are optional arguments, default is false. We want to scale and center prior to PCA so each feature contributes equally to the analysis, not dominated by columns/variables in dataset that have high standard deviation and mean when compared to others just because the units of measurements are on different units/scales.

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

The main PC result figure is called a "score plot" or "PC plot" or "ordination plot"... 

```{r}
library(ggplot2)
ggplot(wisc.pr$x, aes(PC1, PC2, col=diagnosis)) + geom_point()
```


PC1 captures the direction of most variations. We cab score genes based on how much they influence PC1.
PC2 captures the direction with the second most variations.

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27% portion of the original variance is captured by  PC1.

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs are required to describe at least 70% of the variance. (0.72636)

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs are required (0.91010).

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r}
biplot(wisc.pr)
```

The points and names overlap each other. This plot is very hard to understand.

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

This is much more easier to understand. The groups are separated by colors and easier to read.

```{r}
ggplot(wisc.pr$x, aes(PC1, PC3, col=diagnosis)) + geom_point() + labs(x="PC1", y="PC3")
```

## Variance

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
pve <- pr.var/ sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```


> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

The concave.points_mean is row, and PC1 is column for the first principal component.

```{r}
wisc.pr$rotation["concave.points_mean", "PC1"]
```

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

We need 5 PCs to capture more than 80% variance.

```{r}
summary(wisc.pr)
```

## Hierarchical Clustering

Just clustering the original data is not very informative or helpful.

```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(wisc.data)
wisc.hclust <- hclust(data.dist, method="ward.D2")
```

View the clustering

```{r}
plot(wisc.hclust)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=2)
table(wisc.hclust.clusters, diagnosis)
wisc.hclust.clusters2 <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters2, diagnosis)
```

## Combining Methods (PCA and Clustering)

Clustering the original data was not very productive. The PCA results looked promising. Here we combine these methods by clustering from our PCA results. In other words, "clustering in PC space" ...

```{r}
## Take the first three PCs.
dist.pc <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(dist.pc, method="ward.D2")
```

View the tree!

```{r}
plot(wisc.pr.hclust)
abline(h=70, col="red")
```

To get our clustering membership vector (i.e. our main clustering result) we "cut" the tree at a desired height or to yield a desired number of "k" groups.

```{r}
grps <- cutree(wisc.pr.hclust, h=70)
table(grps)
```

How does this clustering grps compare to the expert?

```{r}
table(grps, diagnosis)
```
## Sensitivity/ Specificity

True positive, False negative
Sensitivity: TP/(TP+FN)
Specificity: TN/(TN+FN)

> Q15. How well does the newly created model with four clusters separate out the two diagnoses?

The newly created model with four clusters separated the two diagnoses are better. The results are more "pure" with mostly one diagnosis in one cluster, except for cluster 3.

> Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

```{r}
wisc.km <- kmeans(wisc.data, centers = 2, nstart = 20)
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)
```

Kmeans are doing better than hierarchical clustering because k-means form cluster that successfully separate most of the diagnosis, only mismatched a small number. Cluster 1 has mostly benign and cluster 2 is almost all malignant. Hierarchical clustering has more mixing, so less clearly separated.

## Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
g <- as.factor(grps)
levels(g)
g <- relevel(g, 2)
levels(g)
```


```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q18. Which of these new patients should we prioritize for follow up based on your results?

We should prioritize for follow up for patient 1. This patient's data point locates closer to the black, or malignant cluster, so this new patient is likely malignant.